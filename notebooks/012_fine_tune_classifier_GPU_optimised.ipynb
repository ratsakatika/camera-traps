{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name: NVIDIA A100-SXM4-40GB (4 available)\n",
      "Host name:  gpuhost001.jc.rl.ac.uk\n",
      "User name:  trr26\n",
      "GPU 0 free memory: 37.36 GiB\n",
      "GPU 1 free memory: 38.97 GiB\n",
      "GPU 2 free memory: 38.97 GiB\n",
      "GPU 3 free memory: 38.97 GiB\n",
      "Using GPU: 3\n",
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading images to GPU: 100%|██████████| 17336/17336 [21:45<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading images to GPU: 100%|██████████| 3669/3669 [05:29<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 115/115 [00:33<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Validation Loss: 0.35707510428695494, Initial Validation Accuracy: 91.16925592804579%\n",
      "Overall Precision: 0.9354509494445699, Overall Recall: 0.9116925592804579\n",
      "Wild Boar Precision: 0.9791666666666666, Wild Boar Recall: 0.9359886201991465\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:36<00:00,  1.96it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.22082815971912823, Validation Loss: 0.29100183038769856, Validation Accuracy: 91.95966203325156%\n",
      "Overall Precision: 0.931418682861641, Overall Recall: 0.9195966203325157\n",
      "Wild Boar Precision: 0.9936406995230525, Wild Boar Recall: 0.8890469416785206\n",
      "Saving best epoch...\n",
      "Model saved to ../models/2024-05-28-10-05-53-deepfaune-finetuned-epochs-5-lr-1e-05-wbpenalty-0.0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.0773325218484871, Validation Loss: 0.26516345080801884, Validation Accuracy: 93.43145271191061%\n",
      "Overall Precision: 0.9373844818227234, Overall Recall: 0.934314527119106\n",
      "Wild Boar Precision: 0.9579242636746143, Wild Boar Recall: 0.9715504978662873\n",
      "Saving best epoch...\n",
      "Model saved to ../models/2024-05-28-10-05-53-deepfaune-finetuned-epochs-5-lr-1e-05-wbpenalty-0.0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.051064202051637246, Validation Loss: 0.22646075499145335, Validation Accuracy: 94.73971109294085%\n",
      "Overall Precision: 0.9481746201757243, Overall Recall: 0.9473971109294086\n",
      "Wild Boar Precision: 0.9481582537517054, Wild Boar Recall: 0.9886201991465149\n",
      "Saving best epoch...\n",
      "Model saved to ../models/2024-05-28-10-05-53-deepfaune-finetuned-epochs-5-lr-1e-05-wbpenalty-0.0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:32<00:00,  1.99it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.08146484264907619, Validation Loss: 0.3137011447921395, Validation Accuracy: 91.93240665031344%\n",
      "Overall Precision: 0.9229899634406458, Overall Recall: 0.9193240665031344\n",
      "Wild Boar Precision: 0.9495677233429395, Wild Boar Recall: 0.9374110953058321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.03130040349739705, Validation Loss: 0.23681606195019886, Validation Accuracy: 94.76696647587899%\n",
      "Overall Precision: 0.9497424150148942, Overall Recall: 0.9476696647587899\n",
      "Wild Boar Precision: 0.947945205479452, Wild Boar Recall: 0.984352773826458\n",
      "Saving model's current state...\n",
      "Model saved to ../models/2024-05-28-10-30-54-deepfaune-finetuned-epochs-5-lr-1e-05-wbpenalty-0.0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.19262274690351502, Validation Loss: 0.33920544223823845, Validation Accuracy: 93.75851730716816%\n",
      "Overall Precision: 0.9398390046212476, Overall Recall: 0.9375851730716817\n",
      "Wild Boar Precision: 0.9216467463479415, Wild Boar Recall: 0.9871977240398293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.11287002908341918, Validation Loss: 0.36552460282615834, Validation Accuracy: 93.0771327337149%\n",
      "Overall Precision: 0.9325494353481069, Overall Recall: 0.9307713273371491\n",
      "Wild Boar Precision: 0.8965071151358344, Wild Boar Recall: 0.9857752489331437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.6450039715760876, Validation Loss: 0.744901849621016, Validation Accuracy: 83.892068683565%\n",
      "Overall Precision: 0.8886792148054047, Overall Recall: 0.83892068683565\n",
      "Wild Boar Precision: 0.5937234944868532, Wild Boar Recall: 0.9957325746799431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.3850227367513937, Validation Loss: 0.39250519302238346, Validation Accuracy: 92.99536658490052%\n",
      "Overall Precision: 0.931363648883839, Overall Recall: 0.9299536658490052\n",
      "Wild Boar Precision: 0.910761154855643, Wild Boar Recall: 0.9871977240398293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.3985151276782815, Validation Loss: 0.4294517515753598, Validation Accuracy: 90.56963750340692%\n",
      "Overall Precision: 0.9173725255967452, Overall Recall: 0.9056963750340692\n",
      "Wild Boar Precision: 0.8158508158508159, Wild Boar Recall: 0.9957325746799431\n",
      "Saving model's current state...\n",
      "Model saved to ../models/2024-05-28-10-59-31-deepfaune-finetuned-epochs-10-lr-1e-05-wbpenalty-10.0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 0.05076333156727148, Validation Loss: 0.2504712592354577, Validation Accuracy: 94.03107113654947%\n",
      "Overall Precision: 0.9410591609837562, Overall Recall: 0.9403107113654947\n",
      "Wild Boar Precision: 0.9732016925246827, Wild Boar Recall: 0.9815078236130867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 0.030898158538091733, Validation Loss: 0.33231496744742994, Validation Accuracy: 92.5320250749523%\n",
      "Overall Precision: 0.9316510794116908, Overall Recall: 0.9253202507495231\n",
      "Wild Boar Precision: 0.9398907103825137, Wild Boar Recall: 0.9786628733997155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:30<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 0.020518672279441687, Validation Loss: 0.2627720144185253, Validation Accuracy: 94.0583265194876%\n",
      "Overall Precision: 0.9417679705185843, Overall Recall: 0.940583265194876\n",
      "Wild Boar Precision: 0.9529085872576177, Wild Boar Recall: 0.9786628733997155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss: 0.021918598952709298, Validation Loss: 0.3124650197434399, Validation Accuracy: 93.32243118015808%\n",
      "Overall Precision: 0.9366314370535437, Overall Recall: 0.9332243118015808\n",
      "Wild Boar Precision: 0.9726224783861671, Wild Boar Recall: 0.9601706970128022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 542/542 [04:31<00:00,  2.00it/s]\n",
      "Validation: 100%|██████████| 115/115 [00:24<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 0.02305713367909498, Validation Loss: 0.281918503213503, Validation Accuracy: 94.27636958299264%\n",
      "Overall Precision: 0.9443447118513216, Overall Recall: 0.9427636958299264\n",
      "Wild Boar Precision: 0.9679218967921897, Wild Boar Recall: 0.9871977240398293\n",
      "Saving model's current state...\n",
      "Model saved to ../models/2024-05-28-11-48-53-deepfaune-finetuned-epochs-15-lr-1e-05-wbpenalty-0.0.pt\n",
      "Loading test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading images to GPU: 100%|██████████| 3557/3557 [03:38<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 112/112 [00:23<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.89935338768625%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import InterpolationMode, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import getpass\n",
    "import socket\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Set the PyTorch device (GPU/cuda or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "    device = torch.device(dev)\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_name(torch.device(\"cuda\"))\n",
    "    print(f\"GPU name: {gpu_name} ({torch.cuda.device_count()} available)\")\n",
    "    \n",
    "    print(\"Host name: \", socket.gethostname())  # Retrieve the hostname of the current system to determine the environment\n",
    "    print(\"User name: \", getpass.getuser())  # Retrieve the current user's username\n",
    "\n",
    "    # If the notebook is running on the JASMIN GPU cluster, select the GPU with the most free memory\n",
    "    if socket.gethostname() == \"gpuhost001.jc.rl.ac.uk\":\n",
    "\n",
    "        def select_gpu_with_most_free_memory():\n",
    "            max_memory_available = 0\n",
    "            gpu_id_with_max_memory = 0\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                torch.cuda.set_device(i)\n",
    "                free_mem, total_mem = torch.cuda.mem_get_info(i)\n",
    "                free_mem_gib = free_mem / (1024 ** 3)\n",
    "                free_mem_rounded = round(free_mem_gib, 2)\n",
    "                print(f\"GPU {i} free memory: {free_mem_rounded} GiB\")\n",
    "                if free_mem_gib >= max_memory_available:  # >= biases away from GPU 0, which most JASMIN users default to\n",
    "                    max_memory_available = free_mem_gib\n",
    "                    gpu_id_with_max_memory = i\n",
    "            return gpu_id_with_max_memory\n",
    "\n",
    "        best_gpu = select_gpu_with_most_free_memory()\n",
    "\n",
    "        torch.cuda.set_device(best_gpu)\n",
    "        print(f\"Using GPU: {best_gpu}\")\n",
    "    \n",
    "    else:\n",
    "        _, max_memory = torch.cuda.mem_get_info()\n",
    "        max_memory = max_memory / (1024 ** 3)\n",
    "        print(f\"GPU memory: {max_memory} GiB\")\n",
    "\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "    device = torch.device(dev)\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "gpu_override = False\n",
    "if gpu_override:\n",
    "    torch.cuda.set_device(3)\n",
    "    print(f\"OVERRIDE: Using GPU: {3}\")\n",
    "\n",
    "CROP_SIZE = 182\n",
    "BACKBONE = \"vit_large_patch14_dinov2\"\n",
    "weight_path = \"../models/deepfaune-vit_large_patch14_dinov2.lvd142m.pt\"\n",
    "\n",
    "jasmin = True\n",
    "\n",
    "if jasmin:\n",
    "    train_path = \"../data/split_data/train\"\n",
    "    val_path = \"../data/split_data/val\"\n",
    "    test_path = \"../data/split_data/test\"\n",
    "else:\n",
    "    train_path = \"/media/tom-ratsakatika/CRUCIAL 4TB/FCC Camera Trap Data/split_data/train\"\n",
    "    val_path = \"/media/tom-ratsakatika/CRUCIAL 4TB/FCC Camera Trap Data/split_data/val\"\n",
    "    test_path = \"/media/tom-ratsakatika/CRUCIAL 4TB/FCC Camera Trap Data/split_data/test\"\n",
    "\n",
    "ANIMAL_CLASSES = [\"badger\", \"ibex\", \"red deer\", \"chamois\", \"cat\", \"goat\", \"roe deer\", \"dog\", \"squirrel\", \"equid\", \"genet\",\n",
    "                  \"hedgehog\", \"lagomorph\", \"wolf\", \"lynx\", \"marmot\", \"micromammal\", \"mouflon\",\n",
    "                  \"sheep\", \"mustelid\", \"bird\", \"bear\", \"nutria\", \"fox\", \"wild boar\", \"cow\"]\n",
    "\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None, preload_to_gpu=False):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.preload_to_gpu = preload_to_gpu\n",
    "\n",
    "        for label in os.listdir(directory):\n",
    "            label_dir = os.path.join(directory, label)\n",
    "            if os.path.isdir(label_dir):\n",
    "                for image in os.listdir(label_dir):\n",
    "                    image_path = os.path.join(label_dir, image)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(ANIMAL_CLASSES.index(label))\n",
    "\n",
    "        if self.preload_to_gpu:\n",
    "            self.preload_images()\n",
    "\n",
    "    def preload_images(self):\n",
    "        self.loaded_images = []\n",
    "        for image_path in tqdm(self.images, desc=\"Preloading images to GPU\"):\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            self.loaded_images.append(image.to(device))\n",
    "        self.labels = torch.tensor(self.labels, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload_to_gpu:\n",
    "            return self.loaded_images[idx], self.labels[idx]\n",
    "        else:\n",
    "            image_path = self.images[idx]\n",
    "            label = self.labels[idx]\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, freeze_up_to_layer=16):\n",
    "        super(Classifier, self).__init__()\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = timm.create_model(BACKBONE, pretrained=False, num_classes=len(ANIMAL_CLASSES), dynamic_img_size=True)\n",
    "        state_dict = torch.load(weight_path, map_location=torch.device(device))['state_dict']\n",
    "        self.model.load_state_dict({k.replace('base_model.', ''): v for k, v in state_dict.items()})\n",
    "\n",
    "        # Freeze layers up to the specified layer\n",
    "        if freeze_up_to_layer is not None:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if self._should_freeze_layer(name, freeze_up_to_layer):\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
    "        ])\n",
    "\n",
    "    def _should_freeze_layer(self, name, freeze_up_to_layer):\n",
    "        if 'blocks' in name:\n",
    "            block_num = int(name.split('.')[1])\n",
    "            if block_num <= freeze_up_to_layer:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, image):\n",
    "        img_tensor = self.transforms(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(img_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            top_p, top_class = probabilities.topk(1, dim=1)\n",
    "            return ANIMAL_CLASSES[top_class.item()], top_p.item()\n",
    "\n",
    "# Custom loss function with a higher penalty for misclassifying wild boar\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, penalty_weight=0.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.penalty_weight = penalty_weight\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = self.ce_loss(outputs, targets)\n",
    "        wild_boar_index = ANIMAL_CLASSES.index(\"wild boar\")\n",
    "        wild_boar_mask = (targets == wild_boar_index)\n",
    "        if wild_boar_mask.sum() > 0:\n",
    "            wild_boar_loss = self.ce_loss(outputs[wild_boar_mask], targets[wild_boar_mask])\n",
    "            loss += self.penalty_weight * wild_boar_loss\n",
    "        return loss\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    overall_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    overall_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calculate precision and recall for wild boar\n",
    "    wild_boar_index = ANIMAL_CLASSES.index(\"wild boar\")\n",
    "    wild_boar_precision = precision_score(all_labels, all_preds, labels=[wild_boar_index], average='macro', zero_division=0)\n",
    "    wild_boar_recall = recall_score(all_labels, all_preds, labels=[wild_boar_index], average='macro', zero_division=0)\n",
    "    \n",
    "    return running_loss / len(dataloader), accuracy, overall_precision, overall_recall, wild_boar_precision, wild_boar_recall\n",
    "\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def save_model(model, total_epochs, learning_rate, now, penalty_weight):\n",
    "    model_save_path = f\"../models/{now}-deepfaune-finetuned-epochs-{total_epochs}-lr-{learning_rate}-wbpenalty-{penalty_weight}.pt\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f'Model saved to {model_save_path}')\n",
    "\n",
    "def main():\n",
    "    initial_epochs = 5  # Set the number of epochs\n",
    "    batch_size = 32  # Set the batch size\n",
    "    learning_rate = 1e-5  # Reduced learning rate for fine-tuning\n",
    "    total_epochs = initial_epochs\n",
    "    penalty_weight = 0.0  # Initial penalty weight for wild boar class\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
    "    ])\n",
    "\n",
    "    print('Loading training data...')\n",
    "    train_dataset = AnimalDataset(train_path, transform=transform, preload_to_gpu=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print('Loading validation data...')\n",
    "    val_dataset = AnimalDataset(val_path, transform=transform, preload_to_gpu=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = Classifier(freeze_up_to_layer=16).to(device)  # Freeze up to the 16th layer\n",
    "\n",
    "    criterion = CustomLoss(penalty_weight=penalty_weight)  # Custom loss with initial penalty weight\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize best_val_loss\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Evaluate validation set before training\n",
    "    print('Initial validation evaluation...')\n",
    "    val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall = validate(model, val_loader, criterion, device)\n",
    "    print(f'Initial Validation Loss: {val_loss}, Initial Validation Accuracy: {val_accuracy}%')\n",
    "    print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "    print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    \n",
    "    print('Training started...')\n",
    "    for epoch in range(initial_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall = validate(model, val_loader, criterion, device)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "        print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "        print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Saving best epoch...\")\n",
    "            save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "\n",
    "    if val_loss != best_val_loss:\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        print(\"Saving model's current state...\")\n",
    "        save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "\n",
    "    # Option to continue training\n",
    "    while True:\n",
    "        more_epochs = int(input(\"Enter the number of additional epochs to continue training (0 to stop): \"))\n",
    "        if more_epochs == 0:\n",
    "            break\n",
    "        while True:\n",
    "            learning_rate = float(input(\"Enter the learning rate for the additional epochs (default 1e-5): \"))\n",
    "            if learning_rate <= 1e-4:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Learning rate too high\")\n",
    "        penalty_weight = float(input(\"Enter the penalty weight for wild boar class (default 0): \"))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Update optimizer with new learning rate\n",
    "        criterion = CustomLoss(penalty_weight=penalty_weight)  # Update criterion with new penalty weight\n",
    "        total_epochs += more_epochs\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        \n",
    "        for epoch in range(more_epochs):\n",
    "            train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall = validate(model, val_loader, criterion, device)\n",
    "            print(f'Epoch {total_epochs - more_epochs + epoch + 1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "            print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "            print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                print(\"Saving best epoch...\")\n",
    "                save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "        \n",
    "        if val_loss != best_val_loss:\n",
    "            now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "            print(\"Saving model's current state...\")\n",
    "            save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "\n",
    "    # Load test data\n",
    "    print('Loading test data...')\n",
    "    test_dataset = AnimalDataset(test_path, transform=transform, preload_to_gpu=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Test the model\n",
    "    print('Testing the model...')\n",
    "    test_accuracy = test(model, test_loader, device)\n",
    "    print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "    # Return critical variables for further experimentation\n",
    "    return model, train_loader, val_loader, test_loader, criterion, optimizer, total_epochs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model, train_loader, val_loader, test_loader, criterion, optimizer, total_epochs = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m penalty_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      5\u001b[0m new_weight_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/Boar Balanced PrecisionRecall - 96.8-98.7-deepfaune-finetuned-epochs-15-lr-1e-05-wbpenalty-0for5,10for5,0for5.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m new_model \u001b[38;5;241m=\u001b[39m \u001b[43mClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfreeze_up_to_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Freeze up to the 16th layer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m new_criterion \u001b[38;5;241m=\u001b[39m CustomLoss(penalty_weight\u001b[38;5;241m=\u001b[39mpenalty_weight)  \u001b[38;5;66;03m# Custom loss with initial penalty weight\u001b[39;00m\n\u001b[1;32m      9\u001b[0m new_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(new_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "Cell \u001b[0;32mIn[1], line 132\u001b[0m, in \u001b[0;36mClassifier.__init__\u001b[0;34m(self, freeze_up_to_layer)\u001b[0m\n\u001b[1;32m    130\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(BACKBONE, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(ANIMAL_CLASSES), dynamic_img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 132\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict({k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_model.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Freeze layers up to the specified layer\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state_dict'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "new_weight_path = \"../models/Boar Balanced PrecisionRecall - 96.8-98.7-deepfaune-finetuned-epochs-15-lr-1e-05-wbpenalty-0for5,10for5,0for5.pt\"\n",
    "\n",
    "if 'new_model' in locals():\n",
    "    del new_model\n",
    "\n",
    "new_model = Classifier(freeze_up_to_layer=16).to(device)  # Freeze up to the 16th layer\n",
    "\n",
    "print('Initial validation evaluation...')\n",
    "val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall = validate(new_model, val_loader, criterion, device)\n",
    "print(f'Initial Validation Loss: {val_loss}, Initial Validation Accuracy: {val_accuracy}%')\n",
    "print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "\n",
    "while True:\n",
    "    more_epochs = int(input(\"Enter the number of additional epochs to continue training (0 to stop): \"))\n",
    "    if more_epochs == 0:\n",
    "        break\n",
    "    while True:\n",
    "        learning_rate = float(input(\"Enter the learning rate for the additional epochs (default 1e-5): \"))\n",
    "        if learning_rate <= 1e-4:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Learning rate too high\")\n",
    "    penalty_weight = float(input(\"Enter the penalty weight for wild boar class (default 0): \"))\n",
    "    optimizer = optim.Adam(new_model.parameters(), lr=learning_rate)  # Update optimizer with new learning rate\n",
    "    criterion = CustomLoss(penalty_weight=penalty_weight)  # Update criterion with new penalty weight\n",
    "    total_epochs += more_epochs\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    \n",
    "    for epoch in range(more_epochs):\n",
    "        train_loss = train(new_model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall = validate(new_model, val_loader, criterion, device)\n",
    "        print(f'Epoch {total_epochs - more_epochs + epoch + 1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "        print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "        print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Saving best epoch...\")\n",
    "            save_model(new_model, total_epochs, learning_rate, now, penalty_weight)\n",
    "    \n",
    "    if val_loss != best_val_loss:\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        print(\"Saving model's current state...\")\n",
    "        save_model(new_model, total_epochs, learning_rate, now, penalty_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More thoughts\n",
    "- Change code to save best boar model\n",
    "- Still consider freezing a different number of layers\n",
    "\n",
    "## Thoughts\n",
    "- Decreating learning rate below 1e-6 doesn't help\n",
    "- Next step is to look into number of layers frozen\n",
    "- Perhaps unfreeze all, then slowly increase number of frozen layers? Look into best practice\n",
    "- Otherwise augment dataset - but is that the issue? What tests are thereforre this?\n",
    "- Before augmenting dataset, look at loss function for wild boar instead - likely better resutls - i.e. fine tune for wild boar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
