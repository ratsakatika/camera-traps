{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import InterpolationMode, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import getpass\n",
    "import socket\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Set the PyTorch device (GPU/cuda or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "    device = torch.device(dev)\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_name(torch.device(\"cuda\"))\n",
    "    print(f\"GPU name: {gpu_name} ({torch.cuda.device_count()} available)\")\n",
    "    \n",
    "    print(\"Host name: \", socket.gethostname())  # Retrieve the hostname of the current system to determine the environment\n",
    "    print(\"User name: \", getpass.getuser())  # Retrieve the current user's username\n",
    "\n",
    "    # If the notebook is running on the JASMIN GPU cluster, select the GPU with the most free memory\n",
    "    if socket.gethostname() == \"gpuhost001.jc.rl.ac.uk\":\n",
    "\n",
    "        def select_gpu_with_most_free_memory():\n",
    "            max_memory_available = 0\n",
    "            gpu_id_with_max_memory = 0\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                torch.cuda.set_device(i)\n",
    "                free_mem, total_mem = torch.cuda.mem_get_info(i)\n",
    "                free_mem_gib = free_mem / (1024 ** 3)\n",
    "                free_mem_rounded = round(free_mem_gib, 2)\n",
    "                print(f\"GPU {i} free memory: {free_mem_rounded} GiB\")\n",
    "                if free_mem_gib >= max_memory_available:  # >= biases away from GPU 0, which most JASMIN users default to\n",
    "                    max_memory_available = free_mem_gib\n",
    "                    gpu_id_with_max_memory = i\n",
    "            return gpu_id_with_max_memory\n",
    "\n",
    "        best_gpu = select_gpu_with_most_free_memory()\n",
    "\n",
    "        torch.cuda.set_device(best_gpu)\n",
    "        print(f\"Using GPU: {best_gpu}\")\n",
    "    \n",
    "    else:\n",
    "        _, max_memory = torch.cuda.mem_get_info()\n",
    "        max_memory = max_memory / (1024 ** 3)\n",
    "        print(f\"GPU memory: {max_memory} GiB\")\n",
    "\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "    device = torch.device(dev)\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "gpu_override = False\n",
    "if gpu_override:\n",
    "    torch.cuda.set_device(3)\n",
    "    print(f\"OVERRIDE: Using GPU: {3}\")\n",
    "\n",
    "CROP_SIZE = 182\n",
    "BACKBONE = \"vit_large_patch14_dinov2\"\n",
    "weight_path = \"../models/deepfaune-vit_large_patch14_dinov2.lvd142m.pt\"\n",
    "\n",
    "jasmin = True\n",
    "\n",
    "if jasmin:\n",
    "    train_path = \"../data/split_data/train\"\n",
    "    val_path = \"../data/split_data/val\"\n",
    "    test_path = \"../data/split_data/test\"\n",
    "else:\n",
    "    train_path = \"/media/tom-ratsakatika/CRUCIAL 4TB/FCC Camera Trap Data/split_data/train\"\n",
    "    val_path = \"/media/tom-ratsakatika/CRUCIAL 4TB/FCC Camera Trap Data/split_data/val\"\n",
    "    test_path = \"/media/tom-ratsakatika/CRUCIAL 4TB/FCC Camera Trap Data/split_data/test\"\n",
    "\n",
    "ANIMAL_CLASSES = [\"badger\", \"ibex\", \"red deer\", \"chamois\", \"cat\", \"goat\", \"roe deer\", \"dog\", \"squirrel\", \"equid\", \"genet\",\n",
    "                  \"hedgehog\", \"lagomorph\", \"wolf\", \"lynx\", \"marmot\", \"micromammal\", \"mouflon\",\n",
    "                  \"sheep\", \"mustelid\", \"bird\", \"bear\", \"nutria\", \"fox\", \"wild boar\", \"cow\"]\n",
    "\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None, preload_to_gpu=False):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.preload_to_gpu = preload_to_gpu\n",
    "\n",
    "        for label in os.listdir(directory):\n",
    "            label_dir = os.path.join(directory, label)\n",
    "            if os.path.isdir(label_dir):\n",
    "                for image in os.listdir(label_dir):\n",
    "                    image_path = os.path.join(label_dir, image)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(ANIMAL_CLASSES.index(label))\n",
    "\n",
    "        if self.preload_to_gpu:\n",
    "            self.preload_images()\n",
    "\n",
    "    def preload_images(self):\n",
    "        self.loaded_images = []\n",
    "        for image_path in tqdm(self.images, desc=\"Preloading images to GPU\"):\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            self.loaded_images.append(image.to(device))\n",
    "        self.labels = torch.tensor(self.labels, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload_to_gpu:\n",
    "            return self.loaded_images[idx], self.labels[idx]\n",
    "        else:\n",
    "            image_path = self.images[idx]\n",
    "            label = self.labels[idx]\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, freeze_up_to_layer=16):\n",
    "        super(Classifier, self).__init__()\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = timm.create_model(BACKBONE, pretrained=False, num_classes=len(ANIMAL_CLASSES), dynamic_img_size=True)\n",
    "        state_dict = torch.load(weight_path, map_location=torch.device(device))['state_dict']\n",
    "        self.model.load_state_dict({k.replace('base_model.', ''): v for k, v in state_dict.items()})\n",
    "\n",
    "        # Freeze layers up to the specified layer\n",
    "        if freeze_up_to_layer is not None:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if self._should_freeze_layer(name, freeze_up_to_layer):\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
    "        ])\n",
    "\n",
    "    def _should_freeze_layer(self, name, freeze_up_to_layer):\n",
    "        if 'blocks' in name:\n",
    "            block_num = int(name.split('.')[1])\n",
    "            if block_num <= freeze_up_to_layer:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, image):\n",
    "        img_tensor = self.transforms(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(img_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            top_p, top_class = probabilities.topk(1, dim=1)\n",
    "            return ANIMAL_CLASSES[top_class.item()], top_p.item()\n",
    "\n",
    "# Custom loss function with a higher penalty for misclassifying wild boar\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, penalty_weight=0.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.penalty_weight = penalty_weight\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = self.ce_loss(outputs, targets)\n",
    "\n",
    "        # Indices for bear and wild boar\n",
    "        bear_index = ANIMAL_CLASSES.index(\"bear\")\n",
    "        wild_boar_index = ANIMAL_CLASSES.index(\"wild boar\")\n",
    "\n",
    "        # Masks for bear and wild boar\n",
    "        bear_mask = (targets == bear_index)\n",
    "        wild_boar_mask = (targets == wild_boar_index)\n",
    "\n",
    "        # Combine masks\n",
    "        combined_mask = bear_mask | wild_boar_mask\n",
    "\n",
    "        if combined_mask.sum() > 0:\n",
    "            combined_loss = self.ce_loss(outputs[combined_mask], targets[combined_mask])\n",
    "            loss += self.penalty_weight * combined_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    overall_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    overall_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calculate precision and recall for wild boar\n",
    "    wild_boar_index = ANIMAL_CLASSES.index(\"wild boar\")\n",
    "    wild_boar_precision = precision_score(all_labels, all_preds, labels=[wild_boar_index], average='macro', zero_division=0)\n",
    "    wild_boar_recall = recall_score(all_labels, all_preds, labels=[wild_boar_index], average='macro', zero_division=0)\n",
    "    \n",
    "    # Calculate precision and recall for bear\n",
    "    bear_index = ANIMAL_CLASSES.index(\"bear\")\n",
    "    bear_precision = precision_score(all_labels, all_preds, labels=[bear_index], average='macro', zero_division=0)\n",
    "    bear_recall = recall_score(all_labels, all_preds, labels=[bear_index], average='macro', zero_division=0)\n",
    "    \n",
    "    return (\n",
    "        running_loss / len(dataloader),\n",
    "        accuracy,\n",
    "        overall_precision,\n",
    "        overall_recall,\n",
    "        wild_boar_precision,\n",
    "        wild_boar_recall,\n",
    "        bear_precision,\n",
    "        bear_recall\n",
    "    )\n",
    "\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def save_model(model, total_epochs, learning_rate, now, penalty_weight):\n",
    "    model_save_path = f\"../models/{now}-deepfaune-finetuned-epochs-{total_epochs}-lr-{learning_rate}-wbpenalty-{penalty_weight}.pt\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f'Model saved to {model_save_path}')\n",
    "\n",
    "def main():\n",
    "    initial_epochs = 5  # Set the number of epochs\n",
    "    batch_size = 32  # Set the batch size\n",
    "    learning_rate = 1e-4  # Reduced learning rate for fine-tuning\n",
    "    total_epochs = initial_epochs\n",
    "    penalty_weight = 0.0  # Initial penalty weight for wild boar class\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
    "    ])\n",
    "\n",
    "    print('Loading training data...')\n",
    "    train_dataset = AnimalDataset(train_path, transform=transform, preload_to_gpu=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print('Loading validation data...')\n",
    "    val_dataset = AnimalDataset(val_path, transform=transform, preload_to_gpu=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = Classifier(freeze_up_to_layer=16).to(device)  # Freeze up to the 16th layer\n",
    "\n",
    "    criterion = CustomLoss(penalty_weight=penalty_weight)  # Custom loss with initial penalty weight\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize best_val_loss\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Evaluate validation set before training\n",
    "    print('Initial validation evaluation...')\n",
    "    val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model, val_loader, criterion, device)\n",
    "    print(f'Initial Validation Loss: {val_loss}, Initial Validation Accuracy: {val_accuracy}%')\n",
    "    print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "    print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "    print(f'Bear Precision: {bear_precision}, Bear Recall: {bear_recall}')\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    \n",
    "    print('Training started...')\n",
    "    for epoch in range(initial_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model, val_loader, criterion, device)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "        print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "        print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "        print(f'Bear Precision: {bear_precision}, Bear Recall: {bear_recall}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Saving best epoch...\")\n",
    "            save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "\n",
    "    if val_loss != best_val_loss:\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        print(\"Saving model's current state...\")\n",
    "        save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "\n",
    "    # Option to continue training\n",
    "    while True:\n",
    "        more_epochs = int(input(\"Enter the number of additional epochs to continue training (0 to stop): \"))\n",
    "        if more_epochs == 0:\n",
    "            break\n",
    "        while True:\n",
    "            learning_rate = float(input(\"Enter the learning rate for the additional epochs (default 1e-5): \"))\n",
    "            if learning_rate <= 1e-4:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Learning rate too high\")\n",
    "        penalty_weight = float(input(\"Enter the penalty weight for wild boar class (default 0): \"))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Update optimizer with new learning rate\n",
    "        criterion = CustomLoss(penalty_weight=penalty_weight)  # Update criterion with new penalty weight\n",
    "        total_epochs += more_epochs\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        \n",
    "        for epoch in range(more_epochs):\n",
    "            train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model, val_loader, criterion, device)\n",
    "            print(f'Epoch {total_epochs - more_epochs + epoch + 1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "            print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "            print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "            print(f'Bear Precision: {bear_precision}, Bear Recall: {bear_recall}')\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                print(\"Saving best epoch...\")\n",
    "                save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "        \n",
    "        if val_loss != best_val_loss:\n",
    "            now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "            print(\"Saving model's current state...\")\n",
    "            save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "\n",
    "    # Load test data\n",
    "    print('Loading test data...')\n",
    "    test_dataset = AnimalDataset(test_path, transform=transform, preload_to_gpu=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Test the model\n",
    "    print('Testing the model...')\n",
    "    test_accuracy = test(model, test_loader, device)\n",
    "    print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "    # Return critical variables for further experimentation\n",
    "    return model, train_loader, val_loader, test_loader, criterion, optimizer, total_epochs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model, train_loader, val_loader, test_loader, criterion, optimizer, total_epochs = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weight_path = \"../models/Boar Balanced PrecisionRecall - 96.8-98.7-deepfaune-finetuned-epochs-15-lr-1e-05-wbpenalty-0for5,10for5,0for5.pt\"\n",
    "\n",
    "if 'new_model' in locals():\n",
    "    del new_model\n",
    "\n",
    "new_model = Classifier(freeze_up_to_layer=16).to(device)  # Freeze up to the 16th layer\n",
    "\n",
    "print('Initial validation evaluation...')\n",
    "val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model, val_loader, criterion, device)\n",
    "print(f'Epoch {total_epochs - more_epochs + epoch + 1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "print(f'Bear Precision: {bear_precision}, Bear Recall: {bear_recall}')\n",
    "\n",
    "\n",
    "while True:\n",
    "    more_epochs = int(input(\"Enter the number of additional epochs to continue training (0 to stop): \"))\n",
    "    if more_epochs == 0:\n",
    "        break\n",
    "    while True:\n",
    "        learning_rate = float(input(\"Enter the learning rate for the additional epochs (default 1e-5): \"))\n",
    "        if learning_rate <= 1e-4:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Learning rate too high\")\n",
    "    penalty_weight = float(input(\"Enter the penalty weight for wild boar class (default 0): \"))\n",
    "    optimizer = optim.Adam(new_model.parameters(), lr=learning_rate)  # Update optimizer with new learning rate\n",
    "    criterion = CustomLoss(penalty_weight=penalty_weight)  # Update criterion with new penalty weight\n",
    "    total_epochs += more_epochs\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    \n",
    "    for epoch in range(more_epochs):\n",
    "        train_loss = train(new_model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model, val_loader, criterion, device)\n",
    "        print(f'Epoch {total_epochs - more_epochs + epoch + 1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "        print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "        print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "        print(f'Bear Precision: {bear_precision}, Bear Recall: {bear_recall}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Saving best epoch...\")\n",
    "            save_model(new_model, total_epochs, learning_rate, now, penalty_weight)\n",
    "    \n",
    "    if val_loss != best_val_loss:\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        print(\"Saving model's current state...\")\n",
    "        save_model(new_model, total_epochs, learning_rate, now, penalty_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More thoughts\n",
    "- Change code to save best boar model\n",
    "- Still consider freezing a different number of layers\n",
    "\n",
    "## Thoughts\n",
    "- Decreating learning rate below 1e-6 doesn't help\n",
    "- Next step is to look into number of layers frozen\n",
    "- Perhaps unfreeze all, then slowly increase number of frozen layers? Look into best practice\n",
    "- Otherwise augment dataset - but is that the issue? What tests are thereforre this?\n",
    "- Before augmenting dataset, look at loss function for wild boar instead - likely better resutls - i.e. fine tune for wild boar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
