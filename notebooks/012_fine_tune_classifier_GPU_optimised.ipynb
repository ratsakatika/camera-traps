{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import InterpolationMode, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import getpass\n",
    "import socket\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Set the PyTorch device (GPU/cuda or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "    device = torch.device(dev)\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_name(torch.device(\"cuda\"))\n",
    "    print(f\"GPU name: {gpu_name} ({torch.cuda.device_count()} available)\")\n",
    "    \n",
    "    print(\"Host name: \", socket.gethostname())  # Retrieve the hostname of the current system to determine the environment\n",
    "    print(\"User name: \", getpass.getuser())  # Retrieve the current user's username\n",
    "\n",
    "    # If the notebook is running on the JASMIN GPU cluster, select the GPU with the most free memory\n",
    "    if socket.gethostname() == \"gpuhost001.jc.rl.ac.uk\":\n",
    "\n",
    "        def select_gpu_with_most_free_memory():\n",
    "            max_memory_available = 0\n",
    "            gpu_id_with_max_memory = 0\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                torch.cuda.set_device(i)\n",
    "                free_mem, total_mem = torch.cuda.mem_get_info(i)\n",
    "                free_mem_gib = free_mem / (1024 ** 3)\n",
    "                free_mem_rounded = round(free_mem_gib, 2)\n",
    "                print(f\"GPU {i} free memory: {free_mem_rounded} GiB\")\n",
    "                if free_mem_gib >= max_memory_available:  # >= biases away from GPU 0, which most JASMIN users default to\n",
    "                    max_memory_available = free_mem_gib\n",
    "                    gpu_id_with_max_memory = i\n",
    "            return gpu_id_with_max_memory\n",
    "\n",
    "        best_gpu = select_gpu_with_most_free_memory()\n",
    "\n",
    "        torch.cuda.set_device(best_gpu)\n",
    "        print(f\"Using GPU: {best_gpu}\")\n",
    "    \n",
    "    else:\n",
    "        _, max_memory = torch.cuda.mem_get_info()\n",
    "        max_memory = max_memory / (1024 ** 3)\n",
    "        print(f\"GPU memory: {max_memory} GiB\")\n",
    "\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "    device = torch.device(dev)\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "gpu_override = False\n",
    "if gpu_override:\n",
    "    torch.cuda.set_device(3)\n",
    "    print(f\"OVERRIDE: Using GPU: {3}\")\n",
    "\n",
    "CROP_SIZE = 182\n",
    "BACKBONE = \"vit_large_patch14_dinov2\"\n",
    "weight_path = \"../models/deepfaune-vit_large_patch14_dinov2.lvd142m.pt\"\n",
    "\n",
    "jasmin = True\n",
    "\n",
    "if jasmin:\n",
    "    train_path = \"../data/split_data/train\"\n",
    "    val_path = \"../data/split_data/val\"\n",
    "    test_path = \"../data/split_data/test\"\n",
    "else:\n",
    "    train_path = \"/media/tom-ratsakatika/CRUCIAL 4TB/FCC Camera Trap Data/split_data/train\"\n",
    "    val_path = \"/media/tom-ratsakatika/CRUCIAL 4TB/FCC Camera Trap Data/split_data/val\"\n",
    "    test_path = \"/media/tom-ratsakatika/CRUCIAL 4TB/FCC Camera Trap Data/split_data/test\"\n",
    "\n",
    "ANIMAL_CLASSES = [\"badger\", \"ibex\", \"red deer\", \"chamois\", \"cat\", \"goat\", \"roe deer\", \"dog\", \"squirrel\", \"equid\", \"genet\",\n",
    "                  \"hedgehog\", \"lagomorph\", \"wolf\", \"lynx\", \"marmot\", \"micromammal\", \"mouflon\",\n",
    "                  \"sheep\", \"mustelid\", \"bird\", \"bear\", \"nutria\", \"fox\", \"wild boar\", \"cow\"]\n",
    "\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None, preload_to_gpu=False):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.preload_to_gpu = preload_to_gpu\n",
    "\n",
    "        for label in os.listdir(directory):\n",
    "            label_dir = os.path.join(directory, label)\n",
    "            if os.path.isdir(label_dir):\n",
    "                for image in os.listdir(label_dir):\n",
    "                    image_path = os.path.join(label_dir, image)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(ANIMAL_CLASSES.index(label))\n",
    "\n",
    "        if self.preload_to_gpu:\n",
    "            self.preload_images()\n",
    "\n",
    "    def preload_images(self):\n",
    "        self.loaded_images = []\n",
    "        for image_path in tqdm(self.images, desc=\"Preloading images to GPU\"):\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            self.loaded_images.append(image.to(device))\n",
    "        self.labels = torch.tensor(self.labels, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload_to_gpu:\n",
    "            return self.loaded_images[idx], self.labels[idx]\n",
    "        else:\n",
    "            image_path = self.images[idx]\n",
    "            label = self.labels[idx]\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, freeze_up_to_layer=16):\n",
    "        super(Classifier, self).__init__()\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = timm.create_model(BACKBONE, pretrained=False, num_classes=len(ANIMAL_CLASSES), dynamic_img_size=True)\n",
    "        state_dict = torch.load(weight_path, map_location=torch.device(device))['state_dict']\n",
    "        self.model.load_state_dict({k.replace('base_model.', ''): v for k, v in state_dict.items()})\n",
    "\n",
    "        # Freeze layers up to the specified layer\n",
    "        if freeze_up_to_layer is not None:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if self._should_freeze_layer(name, freeze_up_to_layer):\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
    "        ])\n",
    "\n",
    "    def _should_freeze_layer(self, name, freeze_up_to_layer):\n",
    "        if 'blocks' in name:\n",
    "            block_num = int(name.split('.')[1])\n",
    "            if block_num <= freeze_up_to_layer:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, image):\n",
    "        img_tensor = self.transforms(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(img_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            top_p, top_class = probabilities.topk(1, dim=1)\n",
    "            return ANIMAL_CLASSES[top_class.item()], top_p.item()\n",
    "\n",
    "# Custom loss function with a higher penalty for misclassifying wild boar\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, penalty_weight=0.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.penalty_weight = penalty_weight\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = self.ce_loss(outputs, targets)\n",
    "\n",
    "        # Indices for bear and wild boar\n",
    "        bear_index = ANIMAL_CLASSES.index(\"bear\")\n",
    "        wild_boar_index = ANIMAL_CLASSES.index(\"wild boar\")\n",
    "\n",
    "        # Masks for bear and wild boar\n",
    "        bear_mask = (targets == bear_index)\n",
    "        wild_boar_mask = (targets == wild_boar_index)\n",
    "\n",
    "        # Combine masks\n",
    "        combined_mask = bear_mask | wild_boar_mask\n",
    "\n",
    "        if combined_mask.sum() > 0:\n",
    "            combined_loss = self.ce_loss(outputs[combined_mask], targets[combined_mask])\n",
    "            loss += self.penalty_weight * combined_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    overall_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    overall_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calculate precision and recall for wild boar\n",
    "    wild_boar_index = ANIMAL_CLASSES.index(\"wild boar\")\n",
    "    wild_boar_precision = precision_score(all_labels, all_preds, labels=[wild_boar_index], average='macro', zero_division=0)\n",
    "    wild_boar_recall = recall_score(all_labels, all_preds, labels=[wild_boar_index], average='macro', zero_division=0)\n",
    "    \n",
    "    # Calculate precision and recall for bear\n",
    "    bear_index = ANIMAL_CLASSES.index(\"bear\")\n",
    "    bear_precision = precision_score(all_labels, all_preds, labels=[bear_index], average='macro', zero_division=0)\n",
    "    bear_recall = recall_score(all_labels, all_preds, labels=[bear_index], average='macro', zero_division=0)\n",
    "    \n",
    "    return (\n",
    "        running_loss / len(dataloader),\n",
    "        accuracy,\n",
    "        overall_precision,\n",
    "        overall_recall,\n",
    "        wild_boar_precision,\n",
    "        wild_boar_recall,\n",
    "        bear_precision,\n",
    "        bear_recall\n",
    "    )\n",
    "\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def save_model(model, total_epochs, learning_rate, now, penalty_weight):\n",
    "    model_save_path = f\"../models/{now}-deepfaune-finetuned-epochs-{total_epochs}-lr-{learning_rate}-wbpenalty-{penalty_weight}.pt\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f'Model saved to {model_save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preload Data to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Set the batch size\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
    "])\n",
    "\n",
    "print('Loading training data...')\n",
    "train_dataset = AnimalDataset(train_path, transform=transform, preload_to_gpu=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('Loading validation data...')\n",
    "val_dataset = AnimalDataset(val_path, transform=transform, preload_to_gpu=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 30  # Set the number of epochs\n",
    "total_epochs = initial_epochs\n",
    "learning_rate = 1e-6  # Reduced learning rate for fine-tuning\n",
    "penalty_weight = 0.0  # Initial penalty weight for wild boar class\n",
    "\n",
    "model = Classifier(freeze_up_to_layer=18).to(device)  # Freeze up to the 18th layer\n",
    "\n",
    "criterion = CustomLoss(penalty_weight=penalty_weight)  # Custom loss with initial penalty weight\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize best_val_loss / recall\n",
    "# best_val_loss = float('inf')\n",
    "best_combined_recall = 0\n",
    "\n",
    "# Evaluate validation set before training\n",
    "print('Initial validation evaluation...')\n",
    "val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model, val_loader, criterion, device)\n",
    "print(f'Initial Validation Loss: {val_loss}, Initial Validation Accuracy: {val_accuracy}%')\n",
    "print(f'Initial Overall Precision: {val_precision}, Recall: {val_recall}')\n",
    "print(f'Initial Wild Boar Precision: {wb_precision}, Boar Recall: {wb_recall}')\n",
    "print(f'Initial Bear Precision: {bear_precision}, Recall: {bear_recall}')\n",
    "\n",
    "# Calculate the harmonic mean of the recalls\n",
    "if (wb_recall + bear_recall) == 0:\n",
    "    combined_recall = 0\n",
    "else:\n",
    "    combined_recall = 2 * (wb_recall * bear_recall) / (wb_recall + bear_recall)\n",
    "\n",
    "print(f'Initial Wild Boar & Bear Recall Harmonic Mean: {combined_recall}')\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "print('\\nTraining started...')\n",
    "for epoch in range(initial_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model, val_loader, criterion, device)\n",
    "    print(f'\\nEpoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "    print(f'Overall Precision: {val_precision}, Recall: {val_recall}')\n",
    "    print(f'Wild Boar Precision: {wb_precision}, Recall: {wb_recall}')\n",
    "    print(f'Bear Precision: {bear_precision}, Recall: {bear_recall}')\n",
    "\n",
    "    # Calculate the harmonic mean of the recalls\n",
    "    if (wb_recall + bear_recall) == 0:\n",
    "        combined_recall = 0\n",
    "    else:\n",
    "        combined_recall = 2 * (wb_recall * bear_recall) / (wb_recall + bear_recall)\n",
    "\n",
    "    print(f'Wild Boar & Bear Recall Harmonic Mean: {combined_recall}\\n')\n",
    "\n",
    "    if combined_recall > best_combined_recall:\n",
    "        best_combined_recall = combined_recall\n",
    "        print(f\"Saving best epoch (combined recall harmonic mean = {combined_recall})...\\n\")\n",
    "        save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "\n",
    "# Load test data\n",
    "print('Loading test data...')\n",
    "test_dataset = AnimalDataset(test_path, transform=transform, preload_to_gpu=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test the model\n",
    "print('Testing the model...')\n",
    "test_accuracy = test(model, test_loader, device)\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 30  # Set the number of epochs\n",
    "total_epochs = initial_epochs\n",
    "learning_rate = 1e-6  # Reduced learning rate for fine-tuning\n",
    "penalty_weight = 10.0  # Initial penalty weight for wild boar class\n",
    "\n",
    "model_1 = Classifier(freeze_up_to_layer=18).to(device)  # Freeze up to the 18th layer\n",
    "\n",
    "criterion = CustomLoss(penalty_weight=penalty_weight)  # Custom loss with initial penalty weight\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize best_val_loss / recall\n",
    "# best_val_loss = float('inf')\n",
    "best_combined_recall = 0\n",
    "\n",
    "# Evaluate validation set before training\n",
    "print('Initial validation evaluation...')\n",
    "val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model_1, val_loader, criterion, device)\n",
    "print(f'Initial Validation Loss: {val_loss}, Initial Validation Accuracy: {val_accuracy}%')\n",
    "print(f'Initial Overall Precision: {val_precision}, Recall: {val_recall}')\n",
    "print(f'Initial Wild Boar Precision: {wb_precision}, Boar Recall: {wb_recall}')\n",
    "print(f'Initial Bear Precision: {bear_precision}, Recall: {bear_recall}')\n",
    "\n",
    "# Calculate the harmonic mean of the recalls\n",
    "if (wb_recall + bear_recall) == 0:\n",
    "    combined_recall = 0\n",
    "else:\n",
    "    combined_recall = 2 * (wb_recall * bear_recall) / (wb_recall + bear_recall)\n",
    "\n",
    "print(f'Initial Wild Boar & Bear Recall Harmonic Mean: {combined_recall}')\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "print('\\nTraining started...')\n",
    "for epoch in range(initial_epochs):\n",
    "    train_loss = train(model_1, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model_1, val_loader, criterion, device)\n",
    "    print(f'\\nEpoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "    print(f'Overall Precision: {val_precision}, Recall: {val_recall}')\n",
    "    print(f'Wild Boar Precision: {wb_precision}, Recall: {wb_recall}')\n",
    "    print(f'Bear Precision: {bear_precision}, Recall: {bear_recall}')\n",
    "\n",
    "    # Calculate the harmonic mean of the recalls\n",
    "    if (wb_recall + bear_recall) == 0:\n",
    "        combined_recall = 0\n",
    "    else:\n",
    "        combined_recall = 2 * (wb_recall * bear_recall) / (wb_recall + bear_recall)\n",
    "\n",
    "    print(f'Wild Boar & Bear Recall Harmonic Mean: {combined_recall}\\n')\n",
    "\n",
    "    if combined_recall > best_combined_recall:\n",
    "        best_combined_recall = combined_recall\n",
    "        print(f\"Saving best epoch (combined recall harmonic mean = {combined_recall})...\\n\")\n",
    "        save_model(model_1, total_epochs, learning_rate, now, penalty_weight)\n",
    "\n",
    "# Load test data\n",
    "print('Loading test data...')\n",
    "test_dataset = AnimalDataset(test_path, transform=transform, preload_to_gpu=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test the model\n",
    "print('Testing the model_1...')\n",
    "test_accuracy = test(model_1, test_loader, device)\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to continue training\n",
    "while True:\n",
    "    more_epochs = int(input(\"Enter the number of additional epochs to continue training (0 to stop): \"))\n",
    "    if more_epochs == 0:\n",
    "        break\n",
    "    while True:\n",
    "        learning_rate = float(input(\"Enter the learning rate for the additional epochs (default 1e-5): \"))\n",
    "        if learning_rate <= 1e-4:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Learning rate too high\")\n",
    "    penalty_weight = float(input(\"Enter the penalty weight for wild boar class (default 0): \"))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Update optimizer with new learning rate\n",
    "    criterion = CustomLoss(penalty_weight=penalty_weight)  # Update criterion with new penalty weight\n",
    "    total_epochs += more_epochs\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    \n",
    "    for epoch in range(more_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model, val_loader, criterion, device)\n",
    "        print(f'Epoch {total_epochs - more_epochs + epoch + 1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "        print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "        print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "        print(f'Bear Precision: {bear_precision}, Bear Recall: {bear_recall}')\n",
    "\n",
    "        # Calculate the harmonic mean of the recalls\n",
    "        if (wb_recall + bear_recall) == 0:\n",
    "            combined_recall = 0\n",
    "        else:\n",
    "            combined_recall = 2 * (wb_recall * bear_recall) / (wb_recall + bear_recall)\n",
    "\n",
    "        print(f'Wild Boar & Bear Recall Harmonic Mean: {combined_recall}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Saving best epoch...\")\n",
    "            save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "    \n",
    "    if val_loss != best_val_loss:\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        print(\"Saving model's current state...\")\n",
    "        save_model(model, total_epochs, learning_rate, now, penalty_weight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Fresh Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weight_path = \"../models/deepfaune-vit_large_patch14_dinov2.lvd142m.pt\"\n",
    "\n",
    "if 'new_model' in locals():\n",
    "    del new_model\n",
    "\n",
    "new_model = Classifier(freeze_up_to_layer=16).to(device)  # Freeze up to the 16th layer\n",
    "\n",
    "print('Initial validation evaluation...')\n",
    "val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(new_model, val_loader, criterion, device)\n",
    "print(f'Initial Validation Loss: {val_loss}, Initial Validation Accuracy: {val_accuracy}%')\n",
    "print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "print(f'Bear Precision: {bear_precision}, Bear Recall: {bear_recall}')\n",
    "\n",
    "\n",
    "while True:\n",
    "    more_epochs = int(input(\"Enter the number of additional epochs to continue training (0 to stop): \"))\n",
    "    if more_epochs == 0:\n",
    "        break\n",
    "    while True:\n",
    "        learning_rate = float(input(\"Enter the learning rate for the additional epochs (default 1e-5): \"))\n",
    "        if learning_rate <= 1e-4:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Learning rate too high\")\n",
    "    penalty_weight = float(input(\"Enter the penalty weight for wild boar class (default 0): \"))\n",
    "    optimizer = optim.Adam(new_model.parameters(), lr=learning_rate)  # Update optimizer with new learning rate\n",
    "    criterion = CustomLoss(penalty_weight=penalty_weight)  # Update criterion with new penalty weight\n",
    "    total_epochs += more_epochs\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    \n",
    "    for epoch in range(more_epochs):\n",
    "        train_loss = train(new_model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_accuracy, val_precision, val_recall, wb_precision, wb_recall, bear_precision, bear_recall = validate(model, val_loader, criterion, device)\n",
    "        print(f'Epoch {total_epochs - more_epochs + epoch + 1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "        print(f'Overall Precision: {val_precision}, Overall Recall: {val_recall}')\n",
    "        print(f'Wild Boar Precision: {wb_precision}, Wild Boar Recall: {wb_recall}')\n",
    "        print(f'Bear Precision: {bear_precision}, Bear Recall: {bear_recall}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Saving best epoch...\")\n",
    "            save_model(new_model, total_epochs, learning_rate, now, penalty_weight)\n",
    "    \n",
    "    if val_loss != best_val_loss:\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        print(\"Saving model's current state...\")\n",
    "        save_model(new_model, total_epochs, learning_rate, now, penalty_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Epoch\": [\"Initial\", 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"Train Loss\": [None, 0.2063, 0.0994, 0.0577, 0.0361, 0.0280, 0.0212, 0.0138, 0.0106, 0.0090, 0.0107],\n",
    "    \"Validation Loss\": [0.3571, 0.2203, 0.2086, 0.2102, 0.2013, 0.2008, 0.2048, 0.2084, 0.2061, 0.2064, 0.2217],\n",
    "    \"Validation Accuracy (%)\": [91.1693, 94.2219, 94.4672, 94.2491, 95.0123, 94.7125, 95.2576, 95.2576, 95.1758, 95.4756, 95.1485],\n",
    "    \"Overall Precision\": [0.9355, 0.9456, 0.9466, 0.9442, 0.9527, 0.9490, 0.9543, 0.9542, 0.9537, 0.9557, 0.9534],\n",
    "    \"Overall Recall\": [0.9117, 0.9422, 0.9447, 0.9425, 0.9501, 0.9471, 0.9526, 0.9526, 0.9518, 0.9548, 0.9515],\n",
    "    \"Wild Boar Precision\": [0.9792, 0.9329, 0.9607, 0.9397, 0.9637, 0.9784, 0.9660, 0.9649, 0.9650, 0.9715, 0.9636],\n",
    "    \"Wild Boar Recall\": [0.9360, 0.9886, 0.9744, 0.9758, 0.9829, 0.9673, 0.9687, 0.9772, 0.9801, 0.9701, 0.9801],\n",
    "    \"Bear Precision\": [0.9620, 0.9536, 0.9233, 0.9486, 0.9792, 0.9260, 0.9545, 0.9655, 0.9574, 0.9317, 0.9822],\n",
    "    \"Bear Recall\": [0.9347, 0.9347, 0.9574, 0.9432, 0.9375, 0.9602, 0.9545, 0.9545, 0.9574, 0.9688, 0.9403]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.round(4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
