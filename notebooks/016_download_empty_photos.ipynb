{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Empty Images From the LILA Datasets\n",
    "\n",
    "## Wellington Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the file path\n",
    "file_path = '../data/detector_tests/wellington_camera_traps.csv'\n",
    "print(f\"Loading CSV file from {file_path}\")\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"CSV file loaded successfully.\")\n",
    "\n",
    "# Filter the rows with label 'NOTHINGHERE'\n",
    "print(\"Filtering rows with label 'NOTHINGHERE'.\")\n",
    "nothinghere_files = data[data['label'] == 'NOTHINGHERE']['file']\n",
    "print(f\"Found {len(nothinghere_files)} files with label 'NOTHINGHERE'.\")\n",
    "\n",
    "# Sample 20,000 file names randomly\n",
    "print(\"Sampling 20,000 file names randomly.\")\n",
    "sampled_files = nothinghere_files.sample(n=20000, random_state=42)\n",
    "print(\"Sampled 20,000 file names.\")\n",
    "\n",
    "\n",
    "# Create a directory to save the downloaded images\n",
    "download_dir = 'downloaded_images'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "print(f\"Created directory {download_dir} for downloading images.\")\n",
    "\n",
    "\n",
    "# Download the images with a progress bar\n",
    "base_url = \"https://storage.googleapis.com/public-datasets-lila/wellington-unzipped/images/\"\n",
    "print(\"Starting to download images.\")\n",
    "for file_name in tqdm(sampled_files, desc=\"Downloading images\"):\n",
    "    file_name = file_name.replace('.jpg', '.JPG')\n",
    "    url = base_url + file_name\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(download_dir, file_name), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}\")\n",
    "\n",
    "print(\"Download complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WCS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the JSON file path\n",
    "json_file_path = '../data/detector_tests/wcs_camera_traps.json'\n",
    "print(f\"Loading JSON file from {json_file_path}\")\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"JSON file loaded successfully.\")\n",
    "\n",
    "# Extract \"empty\" category id\n",
    "empty_category_id = None\n",
    "for category in data['categories']:\n",
    "    if category['name'] == 'empty':\n",
    "        empty_category_id = category['id']\n",
    "        break\n",
    "if empty_category_id is None:\n",
    "    print(\"No 'empty' category found in JSON.\")\n",
    "else:\n",
    "    # Extract images with \"empty\" category\n",
    "    print(\"Extracting images with 'empty' category.\")\n",
    "    empty_image_ids = set()\n",
    "    for annotation in tqdm(data['annotations'], desc=\"Extracting image IDs\"):\n",
    "        if annotation['category_id'] == empty_category_id:\n",
    "            empty_image_ids.add(annotation['image_id'])\n",
    "\n",
    "    empty_images = [img for img in data['images'] if img['id'] in empty_image_ids]\n",
    "    print(f\"Found {len(empty_images)} images with 'empty' category.\")\n",
    "\n",
    "    # Randomly sample 20,000 images\n",
    "    if len(empty_images) > 20000:\n",
    "        empty_images = random.sample(empty_images, 20000)\n",
    "    print(f\"Sampled {len(empty_images)} images for download.\")\n",
    "\n",
    "    # Create a directory to save the downloaded images\n",
    "    download_dir = '../data/detector_tests/downloaded_empty_images'\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    print(f\"Created directory {download_dir} for downloading images.\")\n",
    "\n",
    "    # Download the images with a progress bar\n",
    "    base_url = \"https://storage.googleapis.com/public-datasets-lila/wcs-unzipped/\"\n",
    "    print(\"Starting to download images.\")\n",
    "\n",
    "    # Modify the download loop to start from image 1093\n",
    "    start_index = 0\n",
    "    for i, image in enumerate(tqdm(empty_images, desc=\"Downloading images\")):\n",
    "        if i < start_index:\n",
    "            continue\n",
    "        file_name = image['file_name'].lower()\n",
    "        url = f\"{base_url}{file_name}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(download_dir, f\"{i}.jpg\"), 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name} from {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fauna and Flora Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the file path\n",
    "file_path = '../data/detector_tests/Fauna_Flora_Image_Database.csv'\n",
    "print(f\"Loading CSV file from {file_path}\")\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"CSV file loaded successfully.\")\n",
    "\n",
    "# Filter the rows with label 'Blank'\n",
    "print(\"Filtering rows with label 'Blank'.\")\n",
    "blank_file_urls = data[data['common_name'] == 'Blank']['location']\n",
    "print(f\"Found {len(blank_file_urls)} files with label 'Blank'.\")\n",
    "\n",
    "# Sample 20,000 file names randomly\n",
    "print(\"Sampling 19,000 file names randomly.\")\n",
    "sampled_files = blank_file_urls.sample(n=19000, random_state=42)\n",
    "print(\"Sampled 19,000 file names.\")\n",
    "\n",
    "# Create a directory to save the downloaded images\n",
    "download_dir = '../data/detector_tests/Flower_Fauna_Empty_Images'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "print(f\"Created directory {download_dir} for downloading images.\")\n",
    "\n",
    "# Download the images with a progress bar\n",
    "print(\"Starting to download images.\")\n",
    "\n",
    "# Modify the download loop to start from image 1093\n",
    "start_index = 0\n",
    "for i, url in enumerate(tqdm(sampled_files, desc=\"Downloading images\")):\n",
    "    if i < start_index:\n",
    "        continue\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(download_dir, f\"{i}.jpg\"), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://app.wildlifeinsights.org/download/2005775/data-files/28594a77-236b-44ff-8c51-1b1f68fb3f87'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inferrence on the images using MegaDetector and DeepFaune\n",
    "\n",
    "### MegaDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from megadetector.detection.run_detector_batch import load_and_run_detector_batch, write_results_to_file\n",
    "from megadetector.utils import path_utils\n",
    "\n",
    "# Recursively find images in the processed folder\n",
    "\n",
    "image_file_names = path_utils.find_images(processed_folder, recursive=True)\n",
    "\n",
    "sys.stdout = io.StringIO()\n",
    "\n",
    "confidence_threshold = 0.005\n",
    "\n",
    "# Run MegaDetector, default confidence threshold 0.005\n",
    "results = load_and_run_detector_batch('MDV5A', image_file_names, confidence_threshold)\n",
    "\n",
    "# Write results to output file\n",
    "write_results_to_file(results, output_file, relative_path_base=processed_folder, detector_file='MDV5A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFaune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths\n",
    "excel_path = \"../data/detector_tests/original_labelled_photos_file_paths_detections.xlsx\"\n",
    "model_path = \"../models/deepfaune-yolov8s_960.pt\"\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "# Ensure the path components are treated as strings\n",
    "df['Original Path'] = df['Original Path'].astype(str)\n",
    "df['Original Filename'] = df['Original Filename'].astype(str)\n",
    "df['File Extension'] = df['File Extension'].astype(str)\n",
    "\n",
    "# Combine file path, name, and extension to get the full image path\n",
    "df['Full Path'] = df['Original Path'] + '/' + df['Original Filename'] + '.' + df['File Extension']\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(model_path, verbose=False)\n",
    "model.conf = 0.2  # Set confidence threshold\n",
    "\n",
    "# Suppress YOLO logs\n",
    "import logging\n",
    "logging.getLogger('ultralytics').setLevel(logging.WARNING)\n",
    "\n",
    "# Function to process a batch of images\n",
    "def process_batch(image_paths):\n",
    "    results = model(image_paths)\n",
    "    batch_detections = []\n",
    "    batch_confidences = []\n",
    "    for result in results:\n",
    "        if result.boxes is None or len(result.boxes) == 0:\n",
    "            batch_detections.append(\"Empty\")\n",
    "            batch_confidences.append(0)\n",
    "        else:\n",
    "            highest_conf_detection = max(result.boxes, key=lambda x: x.conf[0])  # Access the highest confidence detection\n",
    "            category = highest_conf_detection.cls[0].item()  # Access the class/category index\n",
    "            confidence = highest_conf_detection.conf[0].item()  # Access the confidence score\n",
    "            \n",
    "            if category == 0:\n",
    "                batch_detections.append(\"Animal\")\n",
    "            elif category == 1:\n",
    "                batch_detections.append(\"Human\")\n",
    "            elif category == 2:\n",
    "                batch_detections.append(\"Vehicle\")\n",
    "            else:\n",
    "                batch_detections.append(\"Unknown\")\n",
    "                \n",
    "            batch_confidences.append(confidence)\n",
    "    return batch_detections, batch_confidences\n",
    "\n",
    "# Process images in batches with progress bar\n",
    "batch_size = 32  # 32 = 3.6GB of VRAM\n",
    "image_paths = df['Full Path'].tolist()\n",
    "batch_results = []\n",
    "batch_confidences = []\n",
    "\n",
    "# Initialize progress bar\n",
    "with tqdm(total=len(image_paths), desc=\"Processing images\", unit=\"image\") as pbar:\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch = image_paths[i:i + batch_size]\n",
    "        detections, confidences = process_batch(batch)\n",
    "        batch_results.extend(detections)\n",
    "        batch_confidences.extend(confidences)\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "# Add or update the 'DF_Detector' and 'DF_Detector_Conf' columns in the DataFrame\n",
    "df['DF_Detector'] = batch_results\n",
    "df['DF_Detector_Conf'] = batch_confidences\n",
    "\n",
    "# Save the updated DataFrame back to Excel\n",
    "df.to_excel(excel_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camera-traps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
