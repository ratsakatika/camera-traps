{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Empty Images From the LILA Datasets\n",
    "\n",
    "## Wellington Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the file path\n",
    "file_path = '../data/detector_tests/Wellington_empty_images/wellington_camera_traps.csv'\n",
    "print(f\"Loading CSV file from {file_path}\")\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"CSV file loaded successfully.\")\n",
    "\n",
    "# Filter the rows with label 'NOTHINGHERE'\n",
    "print(\"Filtering rows with label 'NOTHINGHERE'.\")\n",
    "nothinghere_files = data[data['label'] == 'NOTHINGHERE']['file']\n",
    "print(f\"Found {len(nothinghere_files)} files with label 'NOTHINGHERE'.\")\n",
    "\n",
    "# Sample 20,000 file names randomly\n",
    "print(\"Sampling 20,000 file names randomly.\")\n",
    "sampled_files = nothinghere_files.sample(n=20000, random_state=42)\n",
    "print(\"Sampled 20,000 file names.\")\n",
    "\n",
    "\n",
    "# Create a directory to save the downloaded images\n",
    "download_dir = '../data/detector_tests/Wellington_empty_images/'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "print(f\"Created directory {download_dir} for downloading images.\")\n",
    "\n",
    "\n",
    "# Download the images with a progress bar\n",
    "base_url = \"https://storage.googleapis.com/public-datasets-lila/wellington-unzipped/images/\"\n",
    "print(\"Starting to download images.\")\n",
    "for file_name in tqdm(sampled_files, desc=\"Downloading images\"):\n",
    "    file_name = file_name.replace('.jpg', '.JPG')\n",
    "    url = base_url + file_name\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(download_dir, file_name), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}\")\n",
    "\n",
    "print(\"Download complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WCS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the JSON file path\n",
    "json_file_path = '../data/detector_tests/WCS_empty_images/wcs_camera_traps.json'\n",
    "print(f\"Loading JSON file from {json_file_path}\")\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"JSON file loaded successfully.\")\n",
    "\n",
    "# Extract \"empty\" category id\n",
    "empty_category_id = None\n",
    "for category in data['categories']:\n",
    "    if category['name'] == 'empty':\n",
    "        empty_category_id = category['id']\n",
    "        break\n",
    "if empty_category_id is None:\n",
    "    print(\"No 'empty' category found in JSON.\")\n",
    "else:\n",
    "    # Extract images with \"empty\" category\n",
    "    print(\"Extracting images with 'empty' category.\")\n",
    "    empty_image_ids = set()\n",
    "    for annotation in tqdm(data['annotations'], desc=\"Extracting image IDs\"):\n",
    "        if annotation['category_id'] == empty_category_id:\n",
    "            empty_image_ids.add(annotation['image_id'])\n",
    "\n",
    "    empty_images = [img for img in data['images'] if img['id'] in empty_image_ids]\n",
    "    print(f\"Found {len(empty_images)} images with 'empty' category.\")\n",
    "\n",
    "    # Randomly sample 20,000 images\n",
    "    if len(empty_images) > 20000:\n",
    "        empty_images = random.sample(empty_images, 20000)\n",
    "    print(f\"Sampled {len(empty_images)} images for download.\")\n",
    "\n",
    "    # Create a directory to save the downloaded images\n",
    "    download_dir = '../data/detector_tests/WCS_empty_images/'\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    print(f\"Created directory {download_dir} for downloading images.\")\n",
    "\n",
    "    # Download the images with a progress bar\n",
    "    base_url = \"https://storage.googleapis.com/public-datasets-lila/wcs-unzipped/\"\n",
    "    print(\"Starting to download images.\")\n",
    "\n",
    "    # Modify the download loop to start from image 1093\n",
    "    start_index = 0\n",
    "    for i, image in enumerate(tqdm(empty_images, desc=\"Downloading images\")):\n",
    "        if i < start_index:\n",
    "            continue\n",
    "        file_name = image['file_name'].lower()\n",
    "        url = f\"{base_url}{file_name}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(download_dir, f\"{i}.jpg\"), 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name} from {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fauna and Flora Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the file path\n",
    "file_path = '../data/detector_tests/Fauna_Flora_Image_Database.csv'\n",
    "print(f\"Loading CSV file from {file_path}\")\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"CSV file loaded successfully.\")\n",
    "\n",
    "# Filter the rows with label 'Blank'\n",
    "print(\"Filtering rows with label 'Blank'.\")\n",
    "blank_file_urls = data[data['common_name'] == 'Blank']['location']\n",
    "print(f\"Found {len(blank_file_urls)} files with label 'Blank'.\")\n",
    "\n",
    "# Sample 20,000 file names randomly\n",
    "print(\"Sampling 19,000 file names randomly.\")\n",
    "sampled_files = blank_file_urls.sample(n=19000, random_state=42)\n",
    "print(\"Sampled 19,000 file names.\")\n",
    "\n",
    "# Create a directory to save the downloaded images\n",
    "download_dir = '../data/detector_tests/Flower_Fauna_Empty_Images'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "print(f\"Created directory {download_dir} for downloading images.\")\n",
    "\n",
    "# Download the images with a progress bar\n",
    "print(\"Starting to download images.\")\n",
    "\n",
    "# Modify the download loop to start from image 1093\n",
    "start_index = 0\n",
    "for i, url in enumerate(tqdm(sampled_files, desc=\"Downloading images\")):\n",
    "    if i < start_index:\n",
    "        continue\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(download_dir, f\"{i}.jpg\"), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idaho Camera Traps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the JSON data\n",
    "file_path = '../data/detector_tests/Idaho_Empty_Images/idaho-camera-traps.json'\n",
    "save_path = '../data/detector_tests/Idaho_Empty_Images/downloaded_images'\n",
    "print(f\"Loading JSON data from {file_path}...\")\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(\"JSON data loaded successfully.\")\n",
    "\n",
    "# Create a list of file_name for all images where category_id = 0\n",
    "category_id = 0\n",
    "image_file_names = []\n",
    "\n",
    "# Create a dictionary for quick lookup of image file names by image id\n",
    "print(\"Creating a dictionary for quick lookup of image file names by image id...\")\n",
    "image_dict = {image['id']: image['file_name'] for image in data['images']}\n",
    "print(\"Dictionary created successfully.\")\n",
    "\n",
    "# Filter annotations for the required category_id and get the corresponding image file names\n",
    "print(f\"Filtering annotations for category_id = {category_id}...\")\n",
    "for annotation in data['annotations']:\n",
    "    if annotation['category_id'] == category_id:\n",
    "        image_id = annotation['image_id']\n",
    "        if image_id in image_dict:\n",
    "            image_file_names.append(image_dict[image_id])\n",
    "print(f\"Found {len(image_file_names)} images with category_id = {category_id}.\")\n",
    "\n",
    "# Take a random sample of 19,000 images\n",
    "sample_size = min(19000, len(image_file_names))\n",
    "print(f\"Taking a random sample of {sample_size} images...\")\n",
    "sampled_file_names = random.sample(image_file_names, sample_size)\n",
    "print(\"Random sample taken successfully.\")\n",
    "\n",
    "# Generate gsutil commands to download these images\n",
    "print(\"Generating gsutil download commands...\")\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "with open('download_images.sh', 'w') as file:\n",
    "    file.write(\"#!/bin/bash\\n\")\n",
    "    for file_name in sampled_file_names:\n",
    "        # Extract the base file name without the directory structure\n",
    "        base_file_name = os.path.basename(file_name)\n",
    "        gsutil_command = f\"gsutil -q cp gs://public-datasets-lila/idaho-camera-traps/public/{file_name} {save_path}/{base_file_name}\\n\"\n",
    "        file.write(gsutil_command)\n",
    "print(\"Script 'download_images.sh' has been created with gsutil download commands.\")\n",
    "\n",
    "# Run the shell script with a progress bar\n",
    "print(\"Starting the download process with progress bar...\")\n",
    "total_files = len(sampled_file_names)\n",
    "with tqdm(total=total_files, desc=\"Downloading images\", unit=\"file\") as pbar:\n",
    "    with open('download_images.sh', 'r') as file:\n",
    "        for line in file:\n",
    "            os.system(line.strip())\n",
    "            pbar.update(1)\n",
    "\n",
    "print(\"Download process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the JSON data\n",
    "file_path = '../data/detector_tests/Idaho_Empty_Images/idaho-camera-traps.json'\n",
    "save_path = '../data/detector_tests/Idaho_Empty_Images/downloaded_images'\n",
    "print(f\"Loading JSON data from {file_path}...\")\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(\"JSON data loaded successfully.\")\n",
    "\n",
    "# Create a list of file_name for all images where category_id = 0\n",
    "category_id = 0\n",
    "image_file_names = []\n",
    "\n",
    "# Create a dictionary for quick lookup of image file names by image id\n",
    "print(\"Creating a dictionary for quick lookup of image file names by image id...\")\n",
    "image_dict = {image['id']: image['file_name'] for image in data['images']}\n",
    "print(\"Dictionary created successfully.\")\n",
    "\n",
    "# Filter annotations for the required category_id and get the corresponding image file names\n",
    "print(f\"Filtering annotations for category_id = {category_id}...\")\n",
    "for annotation in data['annotations']:\n",
    "    if annotation['category_id'] == category_id:\n",
    "        image_id = annotation['image_id']\n",
    "        if image_id in image_dict:\n",
    "            image_file_names.append(image_dict[image_id])\n",
    "print(f\"Found {len(image_file_names)} images with category_id = {category_id}.\")\n",
    "\n",
    "# Take a random sample of 19,000 images\n",
    "sample_size = min(19000, len(image_file_names))\n",
    "print(f\"Taking a random sample of {sample_size} images...\")\n",
    "sampled_file_names = random.sample(image_file_names, sample_size)\n",
    "print(\"Random sample taken successfully.\")\n",
    "\n",
    "# Ensure the save path exists\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Function to download a single image\n",
    "def download_image(file_name, save_path):\n",
    "    base_file_name = os.path.basename(file_name)\n",
    "    url = f\"https://storage.googleapis.com/public-datasets-lila/idaho-camera-traps/public/{file_name}\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(f\"{save_path}/{base_file_name}\", 'wb') as f:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "    else:\n",
    "        print(f\"Failed to download {url}\")\n",
    "\n",
    "# Download images with progress bar\n",
    "print(\"Starting the download process with progress bar...\")\n",
    "total_files = len(sampled_file_names)\n",
    "with tqdm(total=total_files, desc=\"Downloading images\", unit=\"file\") as pbar:\n",
    "    for file_name in sampled_file_names:\n",
    "        download_image(file_name, save_path)\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"Download process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the JSON data\n",
    "file_path = '../data/detector_tests/Idaho_Empty_Images/idaho-camera-traps.json'\n",
    "save_path = '../data/detector_tests/Idaho_Empty_Images/downloaded_images'\n",
    "print(f\"Loading JSON data from {file_path}...\")\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(\"JSON data loaded successfully.\")\n",
    "\n",
    "# Create a list of file_name for all images where category_id = 0\n",
    "category_id = 0\n",
    "image_file_names = []\n",
    "\n",
    "# Create a dictionary for quick lookup of image file names by image id\n",
    "print(\"Creating a dictionary for quick lookup of image file names by image id...\")\n",
    "image_dict = {image['id']: image['file_name'] for image in data['images']}\n",
    "print(\"Dictionary created successfully.\")\n",
    "\n",
    "# Filter annotations for the required category_id and get the corresponding image file names\n",
    "print(f\"Filtering annotations for category_id = {category_id}...\")\n",
    "for annotation in data['annotations']:\n",
    "    if annotation['category_id'] == category_id:\n",
    "        image_id = annotation['image_id']\n",
    "        if image_id in image_dict:\n",
    "            image_file_names.append(image_dict[image_id])\n",
    "print(f\"Found {len(image_file_names)} images with category_id = {category_id}.\")\n",
    "\n",
    "# Take a random sample of 19,000 images\n",
    "sample_size = min(19000, len(image_file_names))\n",
    "print(f\"Taking a random sample of {sample_size} images...\")\n",
    "sampled_file_names = random.sample(image_file_names, sample_size)\n",
    "print(\"Random sample taken successfully.\")\n",
    "\n",
    "# Ensure the save path exists\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Check which images have already been downloaded\n",
    "downloaded_files = set(os.listdir(save_path))\n",
    "remaining_files = [file_name for file_name in sampled_file_names if os.path.basename(file_name) not in downloaded_files]\n",
    "\n",
    "# Adjust the number of remaining files to ensure the total is 19,000\n",
    "total_downloaded = len(downloaded_files)\n",
    "needed_files = 19000 - total_downloaded\n",
    "remaining_files = remaining_files[:needed_files]\n",
    "\n",
    "print(f\"{total_downloaded} images already downloaded. {len(remaining_files)} images remaining.\")\n",
    "\n",
    "# Function to download a single image\n",
    "def download_image(file_name, save_path):\n",
    "    base_file_name = os.path.basename(file_name)\n",
    "    url = f\"https://storage.googleapis.com/public-datasets-lila/idaho-camera-traps/public/{file_name}\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(f\"{save_path}/{base_file_name}\", 'wb') as f:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to download {url}\")\n",
    "        return False\n",
    "\n",
    "# Download images with progress bar\n",
    "print(\"Starting the download process with progress bar...\")\n",
    "total_files = len(remaining_files)\n",
    "with tqdm(total=total_files, desc=\"Downloading images\", unit=\"file\") as pbar:\n",
    "    for file_name in remaining_files:\n",
    "        if download_image(file_name, save_path):\n",
    "            pbar.update(1)\n",
    "\n",
    "print(\"Download process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inferrence on the images using MegaDetector and DeepFaune\n",
    "\n",
    "### MegaDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from megadetector.detection.run_detector_batch import load_and_run_detector_batch, write_results_to_file\n",
    "from megadetector.utils import path_utils\n",
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "\n",
    "\n",
    "# Define the directory\n",
    "empty_folder = '../data/detector_tests/Idaho_Empty_Images/downloaded_images'\n",
    "\n",
    "# Get the list of files in the directory\n",
    "file_names = os.listdir(empty_folder)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(file_names, columns=[\"File Name\"])\n",
    "\n",
    "# Recursively find images in the processed folder\n",
    "\n",
    "image_file_names = path_utils.find_images(empty_folder, recursive=True)\n",
    "\n",
    "sys.stdout = io.StringIO()\n",
    "\n",
    "confidence_threshold = 0.005\n",
    "\n",
    "# Run MegaDetector, default confidence threshold 0.005\n",
    "results = load_and_run_detector_batch('MDV5A', image_file_names, confidence_threshold)\n",
    "\n",
    "# Initialize columns for category and confidence\n",
    "df['MD_Category'] = None\n",
    "df['MD_Confidence'] = None\n",
    "\n",
    "# Dictionary to map category numbers to names\n",
    "category_map = {'1': 'Animal', '2': 'Human', '3': 'Vehicle'}\n",
    "\n",
    "# Populate the DataFrame\n",
    "for entry in results:\n",
    "    file_name = os.path.basename(entry['file'])\n",
    "    max_confidence = entry['max_detection_conf']\n",
    "    if entry['detections']:\n",
    "        highest_conf_detection = max(entry['detections'], key=lambda x: x['conf'])\n",
    "        category = category_map.get(highest_conf_detection['category'], 'Unknown')\n",
    "    else:\n",
    "        category = 'Empty'\n",
    "    df.loc[df['File Name'] == file_name, 'MD_Category'] = category\n",
    "    df.loc[df['File Name'] == file_name, 'MD_Confidence'] = max_confidence\n",
    "\n",
    "df.to_csv('../data/detector_tests/data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFaune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('../data/detector_tests/data.csv')\n",
    "\n",
    "\n",
    "# Define paths\n",
    "empty_folder = '../data/detector_tests/Idaho_Empty_Images/downloaded_images'\n",
    "model_path = \"../models/deepfaune-yolov8s_960.pt\"\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(model_path, verbose=False)\n",
    "model.conf = 0.005  # Set confidence threshold\n",
    "\n",
    "# Suppress YOLO logs\n",
    "logging.getLogger('ultralytics').setLevel(logging.WARNING)\n",
    "\n",
    "# Function to process a batch of images\n",
    "def process_batch(image_paths):\n",
    "    results = model(image_paths)\n",
    "    batch_detections = []\n",
    "    batch_confidences = []\n",
    "    for result in results:\n",
    "        if result.boxes is None or len(result.boxes) == 0:\n",
    "            batch_detections.append(\"Empty\")\n",
    "            batch_confidences.append(0)\n",
    "        else:\n",
    "            highest_conf_detection = max(result.boxes, key=lambda x: x.conf[0])  # Access the highest confidence detection\n",
    "            category = highest_conf_detection.cls[0].item()  # Access the class/category index\n",
    "            confidence = highest_conf_detection.conf[0].item()  # Access the confidence score\n",
    "            \n",
    "            if category == 0:\n",
    "                batch_detections.append(\"Animal\")\n",
    "            elif category == 1:\n",
    "                batch_detections.append(\"Human\")\n",
    "            elif category == 2:\n",
    "                batch_detections.append(\"Vehicle\")\n",
    "            else:\n",
    "                batch_detections.append(\"Unknown\")\n",
    "                \n",
    "            batch_confidences.append(confidence)\n",
    "    return batch_detections, batch_confidences\n",
    "\n",
    "# Process images in batches with progress bar\n",
    "batch_size = 32  # 32 = 3.6GB of VRAM\n",
    "image_paths = [os.path.join(empty_folder, fname) for fname in df['File Name'].tolist()]\n",
    "batch_results = []\n",
    "batch_confidences = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 18945/18945 [21:34<00:00, 14.64image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO detection results added to the DataFrame and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "df = pd.read_csv('../data/detector_tests/Idaho_Empty_Images/data (3rd copy).csv')\n",
    "\n",
    "# Define paths\n",
    "empty_folder = '../data/detector_tests/Idaho_Empty_Images/downloaded_images'\n",
    "model_path = \"../models/deepfaune-yolov8s_960.pt\"\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(model_path, verbose=False)\n",
    "\n",
    "# Suppress YOLO logs\n",
    "logging.getLogger('ultralytics').setLevel(logging.WARNING)\n",
    "\n",
    "# Function to process a batch of images\n",
    "def process_batch(image_paths):\n",
    "    batch_detections = []\n",
    "    batch_confidences = []\n",
    "    try:\n",
    "        results = model(image_paths, conf=0.005, task='detect')\n",
    "        for result in results:\n",
    "            if result.boxes is None or len(result.boxes) == 0:\n",
    "                batch_detections.append(\"Empty\")\n",
    "                batch_confidences.append(0)\n",
    "            else:\n",
    "                highest_conf_detection = max(result.boxes, key=lambda x: x.conf[0])  # Access the highest confidence detection\n",
    "                category = highest_conf_detection.cls[0].item()  # Access the class/category index\n",
    "                confidence = highest_conf_detection.conf[0].item()  # Access the confidence score\n",
    "\n",
    "                if category == 0:\n",
    "                    batch_detections.append(\"Animal\")\n",
    "                elif category == 1:\n",
    "                    batch_detections.append(\"Human\")\n",
    "                elif category == 2:\n",
    "                    batch_detections.append(\"Vehicle\")\n",
    "                else:\n",
    "                    batch_detections.append(\"Unknown\")\n",
    "\n",
    "                batch_confidences.append(confidence)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        # Log and skip the entire batch if an error occurs\n",
    "        batch_detections.extend([\"Error\"] * len(image_paths))\n",
    "        batch_confidences.extend([0] * len(image_paths))\n",
    "    return batch_detections, batch_confidences\n",
    "\n",
    "# Process images in batches with progress bar\n",
    "batch_size = 32  # 32 = 3.6GB of VRAM\n",
    "image_paths = [os.path.join(empty_folder, fname) for fname in df['File Name'].tolist()]\n",
    "batch_results = []\n",
    "batch_confidences = []\n",
    "\n",
    "# Initialize progress bar\n",
    "with tqdm(total=len(image_paths), desc=\"Processing images\", unit=\"image\") as pbar:\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch = image_paths[i:i + batch_size]\n",
    "        valid_batch = []\n",
    "        for image_path in batch:\n",
    "            try:\n",
    "                # Check if the image can be opened\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.verify()  # Verify that it's an image\n",
    "                valid_batch.append(image_path)\n",
    "            except (OSError, UnidentifiedImageError) as e:\n",
    "                print(f\"Skipping corrupted image: {image_path} - {str(e)}\")\n",
    "                pbar.update(1)  # Update progress bar for skipped images\n",
    "\n",
    "        if valid_batch:\n",
    "            detections, confidences = process_batch(valid_batch)\n",
    "            batch_results.extend(detections)\n",
    "            batch_confidences.extend(confidences)\n",
    "            pbar.update(len(valid_batch))\n",
    "\n",
    "# Add or update the 'DF_Detector' and 'DF_Detector_Conf' columns in the DataFrame\n",
    "df['DF_Detector'] = batch_results\n",
    "df['DF_Detector_Conf'] = batch_confidences\n",
    "\n",
    "# Save the updated DataFrame back to CSV or Excel\n",
    "df.to_csv('../data/detector_tests/data.csv', index=False)\n",
    "\n",
    "print(\"YOLO detection results added to the DataFrame and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>MD_Category</th>\n",
       "      <th>MD_Confidence</th>\n",
       "      <th>DF_Detector</th>\n",
       "      <th>DF_Detector_Conf</th>\n",
       "      <th>Ground_Truth_Detect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc_0067_im_011902.jpg</td>\n",
       "      <td>Empty</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Empty</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loc_0067_im_010802.jpg</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.12200</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.382068</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loc_0008_im_009370.jpg</td>\n",
       "      <td>Empty</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.012061</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loc_0033_im_009075.jpg</td>\n",
       "      <td>Empty</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Empty</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loc_0041_im_006823.jpg</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.01480</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.470771</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18940</th>\n",
       "      <td>loc_0130_im_010631.jpg</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.00548</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.251582</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18941</th>\n",
       "      <td>loc_0018_im_013482.jpg</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.15300</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.049951</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18942</th>\n",
       "      <td>loc_0048_im_006344.jpg</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.01370</td>\n",
       "      <td>Empty</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18943</th>\n",
       "      <td>loc_0027_im_006340.jpg</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.02860</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18944</th>\n",
       "      <td>loc_0052_im_001865.jpg</td>\n",
       "      <td>Animal</td>\n",
       "      <td>0.01240</td>\n",
       "      <td>Empty</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18945 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    File Name MD_Category  MD_Confidence DF_Detector  \\\n",
       "0      loc_0067_im_011902.jpg       Empty        0.00000       Empty   \n",
       "1      loc_0067_im_010802.jpg      Animal        0.12200      Animal   \n",
       "2      loc_0008_im_009370.jpg       Empty        0.00000      Animal   \n",
       "3      loc_0033_im_009075.jpg       Empty        0.00000       Empty   \n",
       "4      loc_0041_im_006823.jpg      Animal        0.01480      Animal   \n",
       "...                       ...         ...            ...         ...   \n",
       "18940  loc_0130_im_010631.jpg      Animal        0.00548      Animal   \n",
       "18941  loc_0018_im_013482.jpg      Animal        0.15300      Animal   \n",
       "18942  loc_0048_im_006344.jpg      Animal        0.01370       Empty   \n",
       "18943  loc_0027_im_006340.jpg      Animal        0.02860      Animal   \n",
       "18944  loc_0052_im_001865.jpg      Animal        0.01240       Empty   \n",
       "\n",
       "       DF_Detector_Conf Ground_Truth_Detect  \n",
       "0              0.000000               Empty  \n",
       "1              0.382068               Empty  \n",
       "2              0.012061               Empty  \n",
       "3              0.000000               Empty  \n",
       "4              0.470771               Empty  \n",
       "...                 ...                 ...  \n",
       "18940          0.251582               Empty  \n",
       "18941          0.049951               Empty  \n",
       "18942          0.000000               Empty  \n",
       "18943          0.016934               Empty  \n",
       "18944          0.000000               Empty  \n",
       "\n",
       "[18945 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/detector_tests/data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camera-traps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
